Predicting Diabetes Using Logistic Regression
ðŸ“Œ Goal

Build a machine-learning model that predicts whether a patient is diabetic based on health measurements such as glucose, BMI, age, blood pressure, insulin, etc.

âš™ï¸ 1. Environment Setup

Python 3.9+ (tested on 3.9.10, also compatible with 3.10/3.12)


Install required libraries:

pip install -r requirements.txt


Recommended tools: VS Code / Jupyter Notebook

ðŸ” 2. Data Exploration (EDA)

Dataset: Pima Indians Diabetes Dataset

Key steps:

df.info(), df.describe() to inspect data

Handle missing values (e.g., blood pressure = 0)

Histograms, boxplots for distributions/outliers

Correlation heatmap with seaborn.heatmap

ðŸ§¹ 3. Data Cleaning

Replace invalid/zero values with median or imputed values

Normalize features using StandardScaler

Split into train/test (80/20 split)

ðŸ¤– 4. Model Building

Baseline: Logistic Regression (sklearn)

Evaluation metrics: accuracy, precision, recall, F1, ROCâ€“AUC

Improvements:

Hyperparameter tuning with GridSearchCV

Compared against Random Forest, XGBoost

### ðŸ“Š Performance Metrics

| Metric                  | Score |
|-------------------------|------:|
| **Accuracy**            | **71%** |
| **Precision**           | 61% |
| **Recall (Sensitivity)**| 52% |
| **F1-score**            | 56% |
| **ROCâ€“AUC**             | 81% |

### ðŸ“Œ Interpretation
- **Accuracy** â†’ Overall fraction of correct predictions  
- **Precision** â†’ Of all predicted diabetics, ~61% were actually diabetic  
- **Recall (Sensitivity)** â†’ Model identified ~52% of the true diabetic cases  
- **F1-score** â†’ Harmonic mean of precision and recall  
- **ROCâ€“AUC** â†’ Probability that the model ranks a random positive higher than a random negative case  

---

### ðŸ“ˆ Visualizations
Below are the final evaluation plots generated from the test set:

| Confusion Matrix | ROC Curve |
|------------------|-----------|
| ![Confusion Matrix](documents/confusion_matrix.png) | ![ROC Curve](documents/roc_curve.png) |


	
ðŸš€ 6. API Integration (FastAPI)

This project now includes an API to make predictions via HTTP requests.

Run the API server:
uvicorn src.api:app --reload


Server will start at: http://127.0.0.1:8000

Docs (Swagger UI): http://127.0.0.1:8000/docs

Example Request (JSON):
{
  "Pregnancies": 2,
  "Glucose": 150,
  "BloodPressure": 85,
  "SkinThickness": 25,
  "Insulin": 100,
  "BMI": 30.5,
  "DiabetesPedigreeFunction": 0.45,
  "Age": 42
}

ðŸ§ª 7. Test Client Script

You can test predictions without Swagger UI using test_client.py:

python test_client.py


Output:

Status code: 200
Response: {"prediction": "Diabetic"}

ðŸ“‚ Project Structure
diabetes-prediction/
â”‚â”€â”€ data/                     # Dataset
â”‚â”€â”€ models/                   # Trained models (.pkl files)
â”‚â”€â”€ notebooks/                # Jupyter notebooks (EDA, training, etc.)
â”‚â”€â”€ src/                      # API + utilities
â”‚   â”œâ”€â”€ api.py                # FastAPI app
â”‚   â”œâ”€â”€ model.py              # Model loading & prediction
â”‚   â””â”€â”€ utils.py              # Helper functions
â”‚â”€â”€ test_client.py            # Python script to test API
â”‚â”€â”€ requirements.txt          # Dependencies
â”‚â”€â”€ README.md                 # Documentation
â”‚â”€â”€ .gitignore                # Ignore unnecessary files

ðŸ“Œ How to Reproduce

Clone this repo:

```bash
git clone https://github.com/Surajkumar123-commits/diabetes-prediction.git
cd diabetes-prediction

Install dependencies

pip install -r requirements.txt

Run Jupyter Notebooks for EDA/model building

Start API server with FastAPI

Test with test_client.py

## ðŸ§  Model Comparison and Results

Three models â€” **Logistic Regression**, **Random Forest**, and **XGBoost** â€” were trained and evaluated on the Diabetes dataset.  
Performance was measured using key classification metrics: **Accuracy**, **Precision**, **Recall**, **F1-Score**, and **ROC AUC**.

| Metric | Logistic Regression | Random Forest | XGBoost |
|:-------|:--------------------:|:--------------:|:--------:|
| **Accuracy** | 0.7078 | 0.7662 | **0.7727** |
| **Precision** | 0.6000 | **0.6957** | 0.6939 |
| **Recall (Sensitivity)** | 0.5000 | 0.5926 | **0.6296** |
| **F1-Score** | 0.5455 | 0.6400 | **0.6602** |
| **ROC AUC** | 0.8074 | **0.8239** | 0.8035 |

---

### ðŸ“Š Interpretation of Results

- **Logistic Regression:**  
  Baseline interpretable model. Simpler but with lower accuracy and recall.

- **Random Forest:**  
  Improved performance, best **ROC AUC (0.8239)**, and robust generalization.

- **XGBoost:**  
  Achieved **highest accuracy (77.3%)**, best **recall**, and overall **strong F1-score**.  
  Ideal for identifying diabetic patients with fewer false negatives.

---

### ðŸ **Conclusion**

> Based on the comparison, **XGBoost** delivers the best overall performance and is recommended for deployment.  
> **Random Forest** is a strong secondary option with excellent AUC performance.  
> **Logistic Regression** remains valuable as a lightweight, interpretable baseline.

---

ðŸ“Ž *Next Step:*  
Model tuning can be performed using **GridSearchCV** or **Optuna** to further optimize hyperparameters for Random Forest and XGBoost.

---

ðŸ”‘ Key points fixed:  
- âœ… Added **triple backticks with `bash`** for commands.  
- âœ… Removed extra blank lines.  
- âœ… Clear step-by-step flow.

---

ðŸ‘‰ If you copy this into your `README.md`, it will render beautifully on GitHub with **highlighted command blocks**.  

Do you want me to generate the **entire README.md final version** (all sections merged with this polish) so you can directly replace your current file?
